{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBL7PGn26oNnktTYdq3CmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RylieWeaver9/Machine-Learning/blob/main/ML_HW_Project_6_Draft_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML HW Project 6**"
      ],
      "metadata": {
        "id": "0KYJkjRQ3R8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary packages"
      ],
      "metadata": {
        "id": "ymlQLnCj3YRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IjVssHcAooeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent optimization\n",
        "# The learning rate is specified by eta\n",
        "class GDOptimizer(object):\n",
        "    def __init__(self, eta):\n",
        "        self.eta = eta\n",
        "\n",
        "    def initialize(self, layers):\n",
        "        pass\n",
        "\n",
        "    # This function performs one gradient descent step\n",
        "    # layers is a list of dense layers in the network\n",
        "    # g is a list of gradients going into each layer before the nonlinear activation\n",
        "    # a is a list of of the activations of each node in the previous layer going in\n",
        "\n",
        "    def update(self, layers, g, a):\n",
        "        m = a[0].shape[1]\n",
        "        for layer, curGrad, curA in zip(layers, g, a):\n",
        "            # TODO: PART F #########################################################################\n",
        "            # Compute the gradients for layer.W and layer.b using the gradient for the output of the\n",
        "            # layer curA and the gradient of the output curGrad\n",
        "            # Use the gradients to update the weight and the bias for the layer\n",
        "            #\n",
        "            # Normalize the learning rate by m (defined above), the number of training examples input\n",
        "            # (in parallel) to the network.\n",
        "            #\n",
        "            # It may help to think about how you would calculate the update if we input just one\n",
        "            # training example at a time; then compute a mean over these individual update values.\n",
        "            # ######################################################################################\n",
        "            # grad_W = (d*vals).dot(a)\n",
        "            # grad_b = (d*vals)\n",
        "            # layer.W = layer.W + grad_W\n",
        "            # layer.b = layer.b + grad_b\n",
        "            pass"
      ],
      "metadata": {
        "id": "ixeV6xyZoq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function used to compute prediction errors\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # Compute the squared error between the prediction yp and the observation y\n",
        "    # This method should compute the cost per element such that the output is the\n",
        "    # same shape as y and yp\n",
        "    @staticmethod\n",
        "    def fx(y,yp):\n",
        "        # TODO: PART B #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return 0.5 * np.linalg.norm(y - yp) ** 2\n",
        "\n",
        "    # Derivative of the cost function with respect to yp\n",
        "    @staticmethod\n",
        "    def dx(y,yp):\n",
        "        # TODO: PART B #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return yp - y "
      ],
      "metadata": {
        "id": "KwE086MGoslk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function fully implemented as an example\n",
        "class SigmoidActivation(object):\n",
        "    @staticmethod\n",
        "    def fx(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def dx(z):\n",
        "        return SigmoidActivation.fx(z) * (1 - SigmoidActivation.fx(z))\n",
        "\n",
        "# Hyperbolic tangent function\n",
        "class TanhActivation(object):\n",
        "\n",
        "    # Compute tanh for each element in the input z\n",
        "    @staticmethod\n",
        "    def fx(z):\n",
        "        # TODO: PART C #################################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return np.tanh(z) \n",
        "\n",
        "    # Compute the derivative of the tanh function with respect to z\n",
        "    @staticmethod\n",
        "    def dx(z):\n",
        "        # TODO: PART C #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return 4 / np.square(np.exp(z) + np.exp(-z))\n",
        "\n",
        "# Rectified linear unit\n",
        "class ReLUActivation(object):\n",
        "    @staticmethod\n",
        "    def fx(z):\n",
        "        # TODO: PART C #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        val = np.copy(z)\n",
        "        val[val < 0] = 0\n",
        "        return val\n",
        "\n",
        "    @staticmethod\n",
        "    def dx(z):\n",
        "        # TODO: PART C #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        val = np.copy(z)\n",
        "        val[val <= 0] = 0\n",
        "        val[val > 0] = 1\n",
        "        return val \n",
        "\n",
        "# Linear activation\n",
        "class LinearActivation(object):\n",
        "    @staticmethod\n",
        "    def fx(z):\n",
        "        # TODO: PART C #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def dx(z):\n",
        "        # TODO: PART C #########################################################################\n",
        "        # Implement me\n",
        "        # ######################################################################################\n",
        "        return np.ones(np.shape(z))"
      ],
      "metadata": {
        "id": "h1O2d7iuo5gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class represents a single hidden or output layer in the neural network\n",
        "class DenseLayer(object):\n",
        "\n",
        "    # numNodes: number of hidden units in the layer\n",
        "    # activation: the activation function to use in this layer\n",
        "    def __init__(self, numNodes, activation):\n",
        "        self.numNodes = numNodes\n",
        "        self.activation = activation\n",
        "\n",
        "    def getNumNodes(self):\n",
        "        return self.numNodes\n",
        "\n",
        "    # Initialize the weight matrix of this layer based on the size of the matrix W\n",
        "    def initialize(self, fanIn, scale=1.0):\n",
        "        s = scale * np.sqrt(6.0 / (self.numNodes + fanIn))\n",
        "        self.W = np.random.normal(0, s,\n",
        "                                   (self.numNodes,fanIn))\n",
        "        self.b = np.random.uniform(-1,1,(self.numNodes,1))\n",
        "\n",
        "    # Apply the activation function of the layer on the input z\n",
        "    def a(self, z):\n",
        "        return self.activation.fx(z)\n",
        "\n",
        "    # Compute the linear part of the layer\n",
        "    # The input a is an n x k matrix where n is the number of samples\n",
        "    # and k is the dimension of the previous layer (or the input to the network)\n",
        "    def z(self, a):\n",
        "        return self.W.dot(a) + self.b # Note, this is implemented where we assume a is k x n\n",
        "\n",
        "    # Compute the derivative of the layer's activation function with respect to z\n",
        "    # where z is the output of the above function.\n",
        "    # This derivative does not contain the derivative of the matrix multiplication\n",
        "    # in the layer.  That part is computed below in the model class.\n",
        "    def dx(self, z):\n",
        "        return self.activation.dx(z)\n",
        "\n",
        "    # Update the weights of the layer by adding dW to the weights\n",
        "    def updateWeights(self, dW):\n",
        "        self.W = self.W + dW\n",
        "\n",
        "    # Update the bias of the layer by adding db to the bias\n",
        "    def updateBias(self, db):\n",
        "        self.b = self.b + db"
      ],
      "metadata": {
        "id": "lP7s72Eqo-sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class handles stacking layers together to form the completed neural network\n",
        "class Model(object):\n",
        "\n",
        "    # inputSize: the dimension of the inputs that go into the network\n",
        "    def __init__(self, inputSize):\n",
        "        self.layers = []\n",
        "        self.inputSize = inputSize\n",
        "\n",
        "    # Add a layer to the end of the network\n",
        "    def addLayer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # Get the output size of the layer at the given index\n",
        "    def getLayerSize(self, index):\n",
        "        if index >= len(self.layers):\n",
        "            return self.layers[-1].getNumNodes()\n",
        "        elif index < 0:\n",
        "            return self.inputSize\n",
        "        else:\n",
        "            return self.layers[index].getNumNodes()\n",
        "\n",
        "    # Initialize the weights of all of the layers in the network and set the cost\n",
        "    # function to use for optimization\n",
        "    def initialize(self, cost, initializeLayers=True):\n",
        "        self.cost = cost\n",
        "        if initializeLayers:\n",
        "            for i in range(0,len(self.layers)):\n",
        "                if i == len(self.layers) - 1:\n",
        "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
        "                else:\n",
        "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
        "\n",
        "    # Compute the output of the network given some input a\n",
        "    # The matrix a has shape n x k where n is the number of samples and\n",
        "    # k is the dimension\n",
        "    # This function returns\n",
        "    # yp - the output of the network\n",
        "    # a - a list of inputs for each layer of the newtork where\n",
        "    #     a[i] is the input to layer i\n",
        "    #     (note this does not include the network output!)\n",
        "    # z - a list of values for each layer after evaluating layer.z(a) but\n",
        "    #     before evaluating the nonlinear function for the layer\n",
        "    def evaluate(self, x):\n",
        "        curA = x.T\n",
        "        a = [curA]\n",
        "        z = []\n",
        "        for layer in self.layers:\n",
        "            z.append(layer.z(curA))\n",
        "            curA = layer.a(z[-1])\n",
        "            a.append(curA)\n",
        "        yp = a.pop()\n",
        "        return yp, a, z\n",
        "\n",
        "    # Compute the output of the network given some input a\n",
        "    # The matrix a has shape n x k where n is the number of samples and\n",
        "    # k is the dimension\n",
        "    def predict(self, a):\n",
        "        a,_,_ = self.evaluate(a)\n",
        "        return a.T\n",
        "\n",
        "    # Computes the gradients at each layer. y is the true labels, yp is the\n",
        "    # predicted labels, and z is a list of the intermediate values in each\n",
        "    # layer. Returns the gradients and the forward pass outputs (per layer).\n",
        "    #\n",
        "    # In particular, we compute dMSE/dz_i. The reasoning behind this is that\n",
        "    # in the update function for the optimizer, we do not give it the z values\n",
        "    # we compute from evaluating the network.\n",
        "    def compute_grad(self, x, y):\n",
        "        # Feed forward, computing outputs of each layer and\n",
        "        # intermediate outputs before the non-linearities\n",
        "        yp, a, z = self.evaluate(x)\n",
        "\n",
        "        # d represents (dMSE / da_i) that you derive in part (e);\n",
        "        #   it is inialized here to be (dMSE / dyp)\n",
        "        d = self.cost.dx(y.T, yp)\n",
        "        grad = []\n",
        "\n",
        "        # Backpropogate the error\n",
        "        for layer, curZ in zip(reversed(self.layers),reversed(z)):\n",
        "            # TODO: PART D #########################################################################\n",
        "            # Compute the gradient of the output of each layer with respect to the error\n",
        "            # grad[i] should correspond with the gradient of the output of layer i\n",
        "            #   before the activation is applied (dMSE / dz_i); be sure values are stored\n",
        "            #   in the correct ordering!\n",
        "            # ######################################################################################\n",
        "            vals = layer.dx(curZ)\n",
        "            # print(vals.shape)\n",
        "            # print(d.shape)\n",
        "            temp = d * vals\n",
        "            # print(temp.shape)\n",
        "            grad = [temp] + grad\n",
        "            # print(layer.W.shape)\n",
        "            d = ((d*vals).T.dot(layer.W)).T\n",
        "\n",
        "        return grad, a\n",
        "\n",
        "    # Computes the gradients at each layer. y is the true labels, yp is the\n",
        "    # predicted labels, and z is a list of the intermediate values in each\n",
        "    # layer. Uses numerical derivatives to solve rather than symbolic derivatives.\n",
        "    # Returns the gradients and the forward pass outputs (per layer).\n",
        "    #\n",
        "    # In particular, we compute dMSE/dz_i. The reasoning behind this is that\n",
        "    # in the update function for the optimizer, we do not give it the z values\n",
        "    # we compute from evaluating the network.\n",
        "    def numerical_grad(self, x, y, delta=1e-4):\n",
        "\n",
        "        # computes the loss function output when starting from the ith layer\n",
        "        # and inputting z_i\n",
        "        def compute_cost_from_layer(layer_i, z_i):\n",
        "            cost = self.layers[layer_i].a(z_i)\n",
        "            for layer in self.layers[layer_i+1:]:\n",
        "                cost = layer.a(layer.z(cost))\n",
        "            return self.cost.fx(y.T, cost)\n",
        "\n",
        "        # numerically computes the gradient of the error with respect to z_i\n",
        "        def compute_grad_from_layer(layer_i, inp):\n",
        "            mask = np.zeros(self.layers[layer_i].b.shape)\n",
        "            grad_z = []\n",
        "            # iterate to compute gradient of each variable in z_i, one at a time\n",
        "            for i in range(mask.shape[0]):\n",
        "                mask[i] = 1\n",
        "                delta_p_output = compute_cost_from_layer(layer_i, inp+mask*delta)\n",
        "                delta_n_output = compute_cost_from_layer(layer_i, inp-mask*delta)\n",
        "                grad_z.append((delta_p_output - delta_n_output) / (2 * delta))\n",
        "                mask[i] = 0;\n",
        "\n",
        "            return np.vstack(grad_z)\n",
        "\n",
        "        _, a, _ = self.evaluate(x)\n",
        "\n",
        "        grad = []\n",
        "        i = 0\n",
        "        curA = x.T\n",
        "        for layer in self.layers:\n",
        "            curA = layer.z(curA)\n",
        "            grad.append(compute_grad_from_layer(i, curA))\n",
        "            curA = layer.a(curA)\n",
        "            i += 1\n",
        "\n",
        "\n",
        "        return grad, a\n",
        "\n",
        "    # Train the network given the inputs x and the corresponding observations y\n",
        "    # The network should be trained for numEpochs iterations using the supplied\n",
        "    # optimizer\n",
        "    def train(self, x, y, numEpochs, optimizer):\n",
        "\n",
        "        # Initialize some stuff\n",
        "        n = x.shape[0]\n",
        "        x = x.copy()\n",
        "        y = y.copy()\n",
        "        hist = []\n",
        "        optimizer.initialize(self.layers)\n",
        "\n",
        "        # Run for the specified number of epochs\n",
        "        for epoch in range(0,numEpochs):\n",
        "\n",
        "            # Compute the gradients\n",
        "            grad, a = self.compute_grad(x, y)\n",
        "\n",
        "            # Update the network weights\n",
        "            optimizer.update(self.layers, grad, a)\n",
        "\n",
        "            # Compute the error at the end of the epoch\n",
        "            yh = self.predict(x)\n",
        "            C = self.cost.fx(y, yh)\n",
        "            C = np.mean(C)\n",
        "            hist.append(C)\n",
        "        return hist"
      ],
      "metadata": {
        "id": "V8PMUzUKpSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing out how multiplications are different"
      ],
      "metadata": {
        "id": "dxtKbL_j3pag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([1,2,3,4])\n",
        "B = np.array([1,4,9,16])\n",
        "print(A*B)\n",
        "print(A.dot(B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsj29kgeQhNF",
        "outputId": "2f71175e-c615-4a08-b225-ed4f5eff7cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  8 27 64]\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KJhWgdlSomH8"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # switch these statements to True to run the code for the corresponding parts\n",
        "    # PART E\n",
        "    DEBUG_MODEL = False\n",
        "    # Part G\n",
        "    BASE_MODEL = False\n",
        "    # Part H\n",
        "    DIFF_SIZES = False\n",
        "    # Part I\n",
        "    RIDGE = False\n",
        "    # Part J\n",
        "    SGD = False\n",
        "\n",
        "\n",
        "\n",
        "    # Generate the training set\n",
        "    np.random.seed(9001)\n",
        "    x=np.random.uniform(-np.pi,np.pi,(1000,1))\n",
        "    y=np.sin(x)\n",
        "    xLin=np.linspace(-np.pi,np.pi,250).reshape((-1,1))\n",
        "    yHats = {}\n",
        "\n",
        "    activations = dict(ReLU=ReLUActivation,\n",
        "                       tanh=TanhActivation,\n",
        "                       linear=LinearActivation)\n",
        "    lr = dict(ReLU=0.8,tanh=0.8,linear=0.8)\n",
        "    names = ['ReLU','linear','tanh']\n",
        "\n",
        "    #### PART F ####\n",
        "    if DEBUG_MODEL:\n",
        "        print('Debugging gradients..')\n",
        "        # Build the model\n",
        "        activation = activations[\"ReLU\"]\n",
        "        model = Model(x.shape[1])\n",
        "        model.addLayer(DenseLayer(10,activation()))\n",
        "        model.addLayer(DenseLayer(10,activation()))\n",
        "        model.addLayer(DenseLayer(1,LinearActivation()))\n",
        "        model.initialize(QuadraticCost())\n",
        "\n",
        "        grad, _ = model.compute_grad(x, y)\n",
        "        n_grad, _ = model.numerical_grad(x, y)\n",
        "        for i in range(len(grad)):\n",
        "            print(n_grad[i].shape)\n",
        "            print('squared difference of layer %d:' % i, np.linalg.norm(grad[i] - n_grad[i]))\n",
        "\n",
        "\n",
        "    #### PART G ####\n",
        "    if BASE_MODEL:\n",
        "        print('\\n----------------------------------------\\n')\n",
        "        print('Standard fully connected network')\n",
        "        for key in names:\n",
        "            # Build the model\n",
        "            activation = activations[key]\n",
        "            model = Model(x.shape[1])\n",
        "            model.addLayer(DenseLayer(100,activation()))\n",
        "            model.addLayer(DenseLayer(100,activation()))\n",
        "            model.addLayer(DenseLayer(1,LinearActivation()))\n",
        "            model.initialize(QuadraticCost())\n",
        "\n",
        "            # Train the model and display the results\n",
        "            hist = model.train(x,y,500,GDOptimizer(eta=lr[key]))\n",
        "            yHat = model.predict(x)\n",
        "            yHats[key] = model.predict(xLin)\n",
        "            error = np.mean(np.square(yHat - y))/2\n",
        "            print(key+' MSE: '+str(error))\n",
        "            plt.plot(hist)\n",
        "            plt.title(key+' Learning curve')\n",
        "            plt.show()\n",
        "\n",
        "        # Plot the approximations\n",
        "        font = {'family' : 'DejaVu Sans',\n",
        "            'weight' : 'bold',\n",
        "                'size'   : 12}\n",
        "        matplotlib.rc('font', **font)\n",
        "        y = np.sin(xLin)\n",
        "        for key in activations:\n",
        "            plt.plot(xLin,y)\n",
        "            plt.plot(xLin,yHats[key])\n",
        "            plt.title(key+' approximation')\n",
        "            plt.savefig(key+'-approx.png')\n",
        "            plt.show()\n",
        "\n",
        "    # Train with different sized networks\n",
        "    #### PART H ####\n",
        "    if DIFF_SIZES:\n",
        "        print('\\n----------------------------------------\\n')\n",
        "        print('Training with various sized network')\n",
        "        names = ['ReLU', 'tanh']\n",
        "        sizes = [5,10,25,50]\n",
        "        widths = [1,2,3]\n",
        "        errors = {}\n",
        "        y = np.sin(x)\n",
        "        for key in names:\n",
        "            error = []\n",
        "            for width in widths:\n",
        "                for size in sizes:\n",
        "                    activation = activations[key]\n",
        "                    model = Model(x.shape[1])\n",
        "                    for _ in range(width):\n",
        "                        model.addLayer(DenseLayer(size,activation()))\n",
        "                    model.addLayer(DenseLayer(1,LinearActivation()))\n",
        "                    model.initialize(QuadraticCost())\n",
        "                    hist = model.train(x,y,500,GDOptimizer(eta=lr[key]))\n",
        "                    yHat = model.predict(x)\n",
        "                    yHats[key] = model.predict(xLin)\n",
        "                    e = np.mean(np.square(yHat - y))/2\n",
        "                    error.append(e)\n",
        "            errors[key] = np.asarray(error).reshape((len(widths),len(sizes)))\n",
        "\n",
        "        # Print the results\n",
        "        for key in names:\n",
        "            error = errors[key]\n",
        "            print(key+' MSE Error')\n",
        "            header = '{:^8}'\n",
        "            for _ in range(len(sizes)):\n",
        "                header += ' {:^8}'\n",
        "            headerText = ['Layers'] + [str(s)+' nodes' for s in sizes]\n",
        "            print(header.format(*headerText))\n",
        "            for width,row in zip(widths,error):\n",
        "                text = '{:>8}'\n",
        "                for _ in range(len(row)):\n",
        "                    text += ' {:<8}'\n",
        "                rowText = [str(width)] + ['{0:.5f}'.format(r) for r in row]\n",
        "                print(text.format(*rowText))\n",
        "\n",
        "    # Perform ridge regression on the last layer of the network\n",
        "    #### PART I ####\n",
        "    if RIDGE:\n",
        "        print('\\n----------------------------------------\\n')\n",
        "        print('Running ridge regression on last layer')\n",
        "        from sklearn.linear_model import Ridge\n",
        "        errors = {}\n",
        "        for key in names:\n",
        "            error = []\n",
        "            sizes = [5,10,25,50]\n",
        "            widths = [1,2,3]\n",
        "            for width in widths:\n",
        "                for size in sizes:\n",
        "                    activation = activations[key]\n",
        "                    model = Model(x.shape[1])\n",
        "                    for _ in range(width):\n",
        "                        model.addLayer(DenseLayer(size,activation()))\n",
        "                    model.initialize(QuadraticCost())\n",
        "                    ridge = Ridge(alpha=0.1)\n",
        "                    X = model.predict(x)\n",
        "                    ridge.fit(X,y)\n",
        "                    yHat = ridge.predict(X)\n",
        "                    e = np.mean(np.square(yHat - y))/2\n",
        "                    error.append(e)\n",
        "            errors[key] = np.asarray(error).reshape((len(widths),len(sizes)))\n",
        "\n",
        "        # Print the results\n",
        "        for key in names:\n",
        "            error = errors[key]\n",
        "            print(key+' MSE Error')\n",
        "            header = '{:^8}'\n",
        "            for _ in range(len(sizes)):\n",
        "                header += ' {:^8}'\n",
        "            headerText = ['Layers'] + [str(s)+' nodes' for s in sizes]\n",
        "            print(header.format(*headerText))\n",
        "            for width,row in zip(widths,error):\n",
        "                text = '{:>8}'\n",
        "                for _ in range(len(row)):\n",
        "                    text += ' {:<8}'\n",
        "                rowText = [str(width)] + ['{0:.5f}'.format(r) for r in row]\n",
        "                print(text.format(*rowText))\n",
        "\n",
        "        # Plot the results\n",
        "        for key in names:\n",
        "            for width,row in zip(widths,error):\n",
        "                layer = ' layers'\n",
        "                if width == 1:\n",
        "                    layer = ' layer'\n",
        "                plt.semilogy(row,label=str(width)+layer)\n",
        "            plt.title('MSE for ridge regression with '+key+' activation (eta=0.8)')\n",
        "            plt.xticks(range(len(sizes)),sizes)\n",
        "            plt.xlabel('Layer size')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.legend()\n",
        "            plt.savefig(key+'-ridge.png')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    #### BONUS PART J ####\n",
        "    if SGD:\n",
        "        print('\\n----------------------------------------\\n')\n",
        "        print('Running SGD regression on last layer')\n",
        "        from sklearn.linear_model import SGDRegressor\n",
        "        errors = {}\n",
        "        for key in names:\n",
        "            error = []\n",
        "            sizes = [5,10,25,50]\n",
        "            widths = [1,2,3]\n",
        "            for width in widths:\n",
        "                for size in sizes:\n",
        "                    activation = activations[key]\n",
        "                    model = Model(x.shape[1])\n",
        "                    for _ in range(width):\n",
        "                        model.addLayer(DenseLayer(size,activation()))\n",
        "                    model.initialize(QuadraticCost())\n",
        "                    ridge = Ridge(alpha=0.1)\n",
        "                    X = model.predict(x)\n",
        "                    ridge.fit(X,y)\n",
        "                    yHat = ridge.predict(X)\n",
        "                    e = np.mean(np.square(yHat - y))/2\n",
        "                    error.append(e)\n",
        "            errors[key] = np.asarray(error).reshape((len(widths),len(sizes)))\n",
        "\n",
        "        # Print the results\n",
        "        for key in names:\n",
        "            error = errors[key]\n",
        "            print(key+' MSE Error')\n",
        "            header = '{:^8}'\n",
        "            for _ in range(len(sizes)):\n",
        "                header += ' {:^8}'\n",
        "            headerText = ['Layers'] + [str(s)+' nodes' for s in sizes]\n",
        "            print(header.format(*headerText))\n",
        "            for width,row in zip(widths,error):\n",
        "                text = '{:>8}'\n",
        "                for _ in range(len(row)):\n",
        "                    text += ' {:<8}'\n",
        "                rowText = [str(width)] + ['{0:.5f}'.format(r) for r in row]\n",
        "                print(text.format(*rowText))\n",
        "\n",
        "        # Plot the results\n",
        "        for key in names:\n",
        "            for width,row in zip(widths,error):\n",
        "                layer = ' layers'\n",
        "                if width == 1:\n",
        "                    layer = ' layer'\n",
        "                plt.semilogy(row,label=str(width)+layer)\n",
        "            plt.title('MSE for SGD regression with '+key+' activation')\n",
        "            plt.xticks(range(len(sizes)),sizes)\n",
        "            plt.xlabel('Layer size')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.legend()\n",
        "            plt.savefig(key+'-ridge.png')\n",
        "            plt.show()"
      ]
    }
  ]
}